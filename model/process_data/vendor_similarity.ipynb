{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9ca91966",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import Utils\n",
    "from tqdm import tqdm \n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4841e1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "utils = Utils(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8f6046",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vals = utils.get_full()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4eea44",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vendors = list(vals.keys())\n",
    "vendors[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a823277c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from geonamescache import GeonamesCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863938b3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "custom_stopwords = {\n",
    "    \"llc\", \"inc\", \"corp\", \"company\", \"corporation\", \"group\", \n",
    "    \"limited\", \"technologies\", \"solutions\", \"systems\", \n",
    "    \"enterprises\", \"international\", \"global\", \"services\",\n",
    "    \"industries\", \"manufacturing\", \"partners\", \"holdings\"\n",
    "}\n",
    "us_states = [\"alabama\", \"alaska\", \"arizona\", \"arkansas\", \"california\", \"colorado\", \"connecticut\", \"delaware\", \"florida\", \"georgia\", \"hawaii\", \"idaho\", \"illinois\", \"indiana\", \"iowa\", \"kansas\", \"kentucky\", \"louisiana\", \"maine\", \"maryland\", \"massachusetts\", \"michigan\", \"minnesota\", \"mississippi\", \"missouri\", \"montana\", \"nebraska\", \"nevada\", \"new hampshire\", \"new jersey\", \"new mexico\", \"new york\", \"north carolina\", \"north dakota\", \"ohio\", \"oklahoma\", \"oregon\", \"pennsylvania\", \"rhode island\", \"south carolina\", \"south dakota\", \"tennessee\", \"texas\", \"utah\", \"vermont\", \"virginia\", \"washington\", \"west virginia\", \"wisconsin\", \"wyoming\"]\n",
    "us_states_abbreviations = [\"al\", \"ak\", \"az\", \"ar\", \"ca\", \"co\", \"ct\", \"de\", \"fl\", \"ga\", \"hi\", \"id\", \"il\", \"in\", \"ia\", \"ks\", \"ky\", \"la\", \"me\", \"md\", \"ma\", \"mi\", \"mn\", \"ms\", \"mo\", \"mt\", \"ne\", \"nv\", \"nh\", \"nj\", \"nm\", \"ny\", \"nc\", \"nd\", \"oh\", \"ok\", \"or\", \"pa\", \"ri\", \"sc\", \"sd\", \"tn\", \"tx\", \"ut\", \"vt\", \"va\", \"wa\", \"wv\", \"wi\", \"wy\"]\n",
    "\n",
    "stop_words.update(custom_stopwords)\n",
    "stop_words.update(us_states)\n",
    "stop_words.update(us_states_abbreviations)\n",
    "gc = GeonamesCache()\n",
    "place_names = set()\n",
    "for place_data in gc.get_cities().values():\n",
    "    place_names.add(place_data[\"name\"].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2186d1e5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_city(word):\n",
    "    tokens = word.split(\" \")\n",
    "    filtered_tokens = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if i + 2 < len(tokens):\n",
    "            three_word_sequence = \" \".join(tokens[i:i+3])\n",
    "            if three_word_sequence in place_names:\n",
    "                i += 3\n",
    "                continue\n",
    "\n",
    "        if i + 1 < len(tokens):\n",
    "            two_word_sequence = \" \".join(tokens[i:i+2])\n",
    "            if two_word_sequence in place_names:\n",
    "                i += 2\n",
    "                continue\n",
    "\n",
    "        if tokens[i] in place_names:\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        filtered_tokens.append(tokens[i])\n",
    "        i += 1\n",
    "\n",
    "    return \" \".join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01e9fbc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def token_and_stem(item):\n",
    "    input_string = item.lower()\n",
    "    no_long_numbers = re.sub(r'\\b\\d{3,}\\b', '', input_string)    \n",
    "    alphanum = re.sub(r'[^a-zA-Z0-9]', ' ', no_long_numbers)\n",
    "    no_places = remove_city(alphanum)\n",
    "    tokens = word_tokenize(no_places)\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "    return ' '.join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15434d3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def function_tester(func, data):\n",
    "    parsed_items = {}\n",
    "\n",
    "    for item in tqdm(data, desc=\"Processing\", unit=\"item\"):\n",
    "        new = func(item)\n",
    "        if new not in parsed_items:\n",
    "            parsed_items[new] = []\n",
    "        parsed_items[new].append(item)\n",
    "    \n",
    "    print(\"Original Length: \", len(data))\n",
    "    print(\"New Length: \", len(parsed_items))\n",
    "    return parsed_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5973bff5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processed_dict = function_tester(token_and_stem, vendors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf1c9dc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_keys = sorted(processed_dict.keys())\n",
    "\n",
    "for key in sorted_keys:\n",
    "    print(f\"{key}: {processed_dict[key]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258a72b7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "potential_matches = {}\n",
    "for word1 in sorted_keys:\n",
    "    potential_matches[word1] = []\n",
    "    for word2 in unique_stems:\n",
    "        similarity_score = fuzz.ratio(word1, word2)\n",
    "        if similarity_score >= 80 and word1 != word2:\n",
    "            potential_matches[word1].append(word2)\n",
    "potential_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3127bd-2ee1-4fc7-9062-1e4d85aab24a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "embeddings = []\n",
    "for word in sorted_keys:\n",
    "    inputs = tokenizer(word, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings.append(outputs[0][0][0].numpy())\n",
    "\n",
    "cosine_similarities = cosine_similarity(embeddings, embeddings)\n",
    "similarity_threshold = 0.9\n",
    "\n",
    "potential_matches = {}\n",
    "for i, word1 in enumerate(sorted_keys):\n",
    "    potential_matches[word1] = []\n",
    "    for j, word2 in enumerate(sorted_keys):\n",
    "        if cosine_similarities[i][j] >= similarity_threshold and word1 != word2:\n",
    "            potential_matches[word1].append(word2)\n",
    "potential_matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e99f7f-4570-47b7-838a-4f4d88323188",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "potential_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddfe36b-fda5-49d7-9707-7dde79121d7e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (ipykernel)",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": {
    "debugger": true
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "name": "vendor_similarity.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
