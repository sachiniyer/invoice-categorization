{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc0c99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import boto3\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "import time\n",
    "\n",
    "from dotenv import dotenv_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4fa0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_vars = dotenv_values('.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b9edd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN=512\n",
    "BATCH_SIZE=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c313cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e6bb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"sample.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90431791",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['Final Mapping'].str.isupper()]\n",
    "label = pd.get_dummies(df['Final Mapping']).apply(lambda row: np.array(row.astype(int)), axis=1)\n",
    "df = df.rename(columns={'Original Vendor': 'vendor', \n",
    "                        'GL Account Description': 'description',\n",
    "                        'Vendor Mapping': 'mapping'})\n",
    "df = df[[\"vendor\", \"description\", \"mapping\"]]\n",
    "df = df.assign(label=label)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc991c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = env_vars['CUSTOM_SEARCH_API_KEY']\n",
    "search_engine_id = env_vars['SEARCH_ENGINE_ID']\n",
    "\n",
    "def get_search(term):\n",
    "    url = f'https://www.googleapis.com/customsearch/v1?key={api_key}&cx={search_engine_id}&q={term}'\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    res = \"\"\n",
    "    if 'items' in data:\n",
    "        for item in data['items']:\n",
    "            if 'title' in item:\n",
    "                res += item['title'] + \" \"\n",
    "            if 'snippet' in item: \n",
    "                res += item['snippet']+ \" \"\n",
    "    return res\n",
    "\n",
    "get_search(\"google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed7656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(\n",
    "    aws_access_key_id=env_vars['AWS_API_KEY'],\n",
    "    aws_secret_access_key=env_vars['AWS_API_SECRET'],\n",
    "    region_name=env_vars['AWS_REGION']\n",
    ")\n",
    "\n",
    "dynamodb_client = session.client('dynamodb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf179e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table(term):\n",
    "    try:\n",
    "        key = {\n",
    "            'name': {'S': term}\n",
    "        }\n",
    "        response = dynamodb_client.get_item(\n",
    "            TableName=env_vars['AWS_TABLE_NAME'],\n",
    "            Key=key\n",
    "        )\n",
    "        item = response.get('Item')\n",
    "        if item:\n",
    "            return item[\"content\"][\"S\"]\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "print(get_table(\"google\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091aafc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_table(term, content):\n",
    "    try:\n",
    "        item = {\n",
    "            'name': {'S': term},\n",
    "            'content': {'S': content},\n",
    "        }\n",
    "        response = dynamodb_client.put_item(\n",
    "            TableName=env_vars['AWS_TABLE_NAME'],\n",
    "            Item=item\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "put_table(\"google\", get_search(\"google\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc887ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_object(vendor):\n",
    "    table_res = get_table(vendor)\n",
    "    if table_res:\n",
    "        return table_res\n",
    "    search_res = get_search(vendor)\n",
    "    put_table(vendor, search_res)\n",
    "    return search_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678c2dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "df['search'] = df['vendor'].progress_apply(lambda x: get_object(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93044e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VendorDF(Dataset):\n",
    "    def __init__(self, df, max_len):\n",
    "        self.df = df\n",
    "        self.vendors = df.vendor\n",
    "        self.descriptions = df.description\n",
    "        self.mappings = df.mapping\n",
    "        self.searches = df.search\n",
    "        self.targets = df.label\n",
    "        \n",
    "        self.max_len = max_len\n",
    "        \n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        \n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        tokens = text.split()\n",
    "        tokens = [token for token in tokens if token.lower() not in self.stopwords]\n",
    "\n",
    "        lemmatized_tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "        stemmed_tokens = [self.stemmer.stem(token) for token in lemmatized_tokens]\n",
    "\n",
    "        processed_text = \"\".join([token for token in stemmed_tokens if token not in string.punctuation])\n",
    "\n",
    "        return processed_text\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        tokenized_text = self.tokenizer.encode_plus(text,\n",
    "                                                    add_special_tokens=True,\n",
    "                                                    padding='max_length',\n",
    "                                                    truncation=True,\n",
    "                                                    max_length=self.max_len,\n",
    "                                                    return_tensors='pt')\n",
    "        return tokenized_text\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        vendor = self.vendors[index]\n",
    "        description = self.descriptions[index]\n",
    "        mapping = self.mappings[index]\n",
    "        search = self.searches[index]\n",
    "        target = self.targets[index]\n",
    "        \n",
    "        vendor_tokens = self.encode_text(self.clean_text(vendor))\n",
    "        description_tokens = self.encode_text(self.clean_text(description))\n",
    "        mapping_tokens = self.encode_text(self.clean_text(mapping))\n",
    "        search_tokens = self.encode_text(self.clean_text(search))\n",
    "        \n",
    "        vendor_input_ids = vendor_tokens['input_ids'].squeeze(0)\n",
    "        vendor_attention_mask = vendor_tokens['attention_mask'].squeeze(0)\n",
    "\n",
    "        description_input_ids = description_tokens['input_ids'].squeeze(0)\n",
    "        description_attention_mask = description_tokens['attention_mask'].squeeze(0)\n",
    "\n",
    "        mapping_input_ids = mapping_tokens['input_ids'].squeeze(0)\n",
    "        mapping_attention_mask = mapping_tokens['attention_mask'].squeeze(0)\n",
    "        \n",
    "        search_input_ids = search_tokens['input_ids'].squeeze(0)\n",
    "        search_attention_mask = search_tokens['attention_mask'].squeeze(0)\n",
    "        input_ids = [vendor_input_ids, description_input_ids,\n",
    "                                 mapping_input_ids, search_input_ids]\n",
    "        attention_masks = [vendor_attention_mask, description_attention_mask,\n",
    "                                       mapping_attention_mask, search_attention_mask]\n",
    "        \n",
    "        return input_ids, attention_masks, torch.FloatTensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea9d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(3072, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_masks):\n",
    "        \n",
    "        outputs1 = self.bert(input_ids=input_ids[0], attention_mask=attention_masks[0])\n",
    "        outputs2 = self.bert(input_ids=input_ids[1], attention_mask=attention_masks[1])\n",
    "        outputs3 = self.bert(input_ids=input_ids[2], attention_mask=attention_masks[2])\n",
    "        outputs4 = self.bert(input_ids=input_ids[3], attention_mask=attention_masks[3])\n",
    "\n",
    "        pooled_output1 = outputs1.pooler_output\n",
    "        pooled_output2 = outputs2.pooler_output\n",
    "        pooled_output3 = outputs3.pooler_output\n",
    "        pooled_output4 = outputs4.pooler_output\n",
    "\n",
    "        pooled_output = torch.cat((pooled_output1, \n",
    "                                   pooled_output2, \n",
    "                                   pooled_output3, \n",
    "                                   pooled_output4), dim=1)\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e36af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, criterion, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
    "        for batch in pbar:            \n",
    "            input_ids, attention_masks, labels = batch\n",
    "            outputs = model(input_ids, attention_masks)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            pbar.set_postfix({'Loss': loss.item()})\n",
    "            \n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1} Loss: {epoch_loss:.4f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f5870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d88509",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VendorDF(train_df, MAX_LEN)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "num_classes = len(train_df.loc[0, 'label'])\n",
    "model = BERTClassifier(num_classes)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96dc2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bebfa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(model, dataloader, criterion, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bc8c88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
